mobilePhonePredictConfusion <- table(mobilePhoneTesting$CancelledService,
mobilePhonePredict)
print(mobilePhonePredictConfusion)
# Calculate the false positive rate
mobilePhonePredictConfusion[1,2]/
(mobilePhonePredictConfusion[1,2] + mobilePhonePredictConfusion[1,1])
# Calculate the false negative rate
mobilePhonePredictConfusion[2,1]/
(mobilePhonePredictConfusion[2,1] + mobilePhonePredictConfusion[2,2])
# Calculate the model prediction accuracy
sum(diag(mobilePhonePredictConfusion)/nrow(mobilePhoneTesting))
# Medha Singh
# KNN
# Import SedanSize dataset and generate k-Nearest Neighbors model and find the
# best fitting k value
# Install the tidyverse package
# install.packages("tidyverse")
# load the required library, class library was loaded for the prediction code
library(tidyverse)
library(class)
# Set the working directory to Lab07 folder
setwd("C:/Users/ual-laptop/Documents/GitHub/KNN")
# Read SedanSize.csv into a tibble called sedanSize
sedanSize <- read_csv(file = "SedanSize.csv",
col_types = "cfnii",
col_names = TRUE)
# Display sedanSize in the console
print(sedanSize)
# Display the structure of sedanSize in the console
str(sedanSize)
# Display the summary of sedanSize in the console
print(summary(sedanSize))
# Remove the MakeModel feature from the tibble, since it is information-only and
# will not be used as a variable in our model
sedanSize <- sedanSize %>%
select(-MakeModel)
# Separate the tibble into two. One with just the label and one with the other
# variables.
sedanSizeLabels <- sedanSize %>% select(SedanSize)
sedanSize <- sedanSize %>% select(-SedanSize)
# Recreate the displayAllHistograms() function
displayAllHistograms <- function(tibbleDataSet) {
tibbleDataSet %>%
keep(is.numeric) %>%
gather() %>%
ggplot() + geom_histogram(mapping = aes(x = value, fill = key),
color = "black") +
facet_wrap(~ key, scales = "free") +
theme_minimal()
}
# Call the displayAllHistgoram() functions using our sedanSize tibble
displayAllHistograms(sedanSize)
# Randomly split the dataset into sedanSizeTraining (75% of records) and
# sedanSizeTesting (25% of records) using 517 as the random seed
set.seed(517)
sampleSet <- sample(nrow(sedanSize),
round(nrow(sedanSize)*0.75),
replace = FALSE)
sedanSizeTraining <- sedanSize[sampleSet,]
sedanSizeTrainingLabels <- sedanSizeLabels[sampleSet,]
sedanSizeTesting <- sedanSize[-sampleSet,]
sedanSizeTestingLabels <- sedanSizeLabels[-sampleSet,]
# Generate the k-nearest neighbors model using sedanSizeTraining as the train
# argument, sedanSizeTesting as the test argument,
# sedanSizeTrainingLabels$SedanSize as the cl argument, and 7 as the value for
# the k argument
sedanSizePrediction <- knn(train = sedanSizeTraining,
test = sedanSizeTesting,
cl = sedanSizeTrainingLabels$SedanSize,
k = 7)
# Display the predictions from the testing dataset on the console
print(sedanSizePrediction)
# Display summary of the predictions from the testing dataset
print(summary(sedanSizePrediction))
# Evaluate the model by forming a confusion matrix
sedanSizeConfusionMatrix <- table(sedanSizeTestingLabels$SedanSize,
sedanSizePrediction)
# Display the confusion matrix on the console
print(sedanSizeConfusionMatrix)
# Calculate the model predictive accuracy and store it into a variable called
# predictiveAccuracy
predictiveAccuracy <- sum(diag(sedanSizeConfusionMatrix)) /
nrow(sedanSizeTesting)
# Display the predictive accuracy on the console
print(predictiveAccuracy)
# Create a matrix of k-values with their predictive accuracy (the matrix will be
# empty and have 2 columns and 0 rows). Store the matrix into an object called
# kValueMatrix
kValueMatrix <- matrix(data = NA,
nrow = 0,
ncol = 2)
# Assign column names of "k value" and "Predictive accuracy" to the kValueMatrix
colnames(kValueMatrix) <- c("k value", "Predictive accuracy")
# Loop through odd values of k from 1 up to the number of records in the
# training dataset to determine the best fitting model. With each pass through
# the loop, store the k-value along with its predictive accuracy
for (kValue in 1:nrow(sedanSizeTraining)) {
# Only calculate predictive accuracy if the k value is odd
if(kValue %% 2 != 0) {
# Generate the model
sedanSizePrediction <- knn(train = sedanSizeTraining,
test = sedanSizeTesting,
cl = sedanSizeTrainingLabels$SedanSize,
k = kValue)
# Generate the confusion matrix
sedanSizeConfusionMatrix <- table(sedanSizeTestingLabels$SedanSize,
sedanSizePrediction)
# Calculate the predictive accuracy
predictiveAccuracy <- sum(diag(sedanSizeConfusionMatrix)) /
nrow(sedanSizeTesting)
# Add a new row to the the kValueMatrix
kValueMatrix <- rbind(kValueMatrix, c(kValue, predictiveAccuracy))
}
}
# Display the kValueMatrix on the console to determine the best k-value
print(kValueMatrix)
# Medha Singh
# Naive Bayes
# Description: This code takes data from a CSV, displays the data and its structure,
# summarizes the data and splits it into a training and testing dataset using 75%
# and 25% of the data respectively, generates a Naïve Bayes model for the data,
# calculates the probabilities for each observation in the dataset, makes class
# predictions for each observation, and them finally produces a confusion matrix for
# the model and calculates the model’s predictive accuracy
# Install the tidyverse and e1071 packages
# install.packages("tidyverse")
# install.packages("e1071")
# load the required library
library(tidyverse)
library(e1071)
# Set the working directory to Lab07 folder
setwd("C:/Users/ual-laptop/Documents/GitHub/NaiveBayes")
# Read DwellingType.csv into a tibble called dwellingType
dwellingType <- read_csv(file = "DwellingType.csv",
col_types = "filll",
col_names = TRUE)
# Display dwellingType in the console
print(dwellingType)
# Display the structure of dwellingType in the console
print(str(dwellingType))
# Display the summary of dwellingType in the console
print(summary(dwellingType))
# Randomly split the dataset into dwellingTypeTraining (75% of records) and
# dwellingTypeTesting (25% of records) using 154 as the random seed
set.seed(154)
sampleSet <- sample(nrow(dwellingType),
round(nrow(dwellingType)*0.75),
replace = FALSE)
dwellingTypeTraining <- dwellingType[sampleSet,]
dwellingTypeTesting <- dwellingType[-sampleSet,]
# Generate the Naive Bayes model to predict DwellingType based on the other
# variables in the dataset
dwellingTypeModel <- naiveBayes( formula = DwellingType ~ . ,
data = dwellingTypeTraining,
laplace = 1)
# Build probabilities for each record in the testing dataset and store them in
# dwellingTypeProbability
dwellingTypeProbability <- predict(dwellingTypeModel,
dwellingTypeTraining,
type = "raw")
# Display dwellingTypeProbability on the console
print(dwellingTypeProbability)
# Predict classes for each record in the testing dataset and store them in
# dwellingTypePrediction
dwellingTypePrediction <- predict(dwellingTypeModel,
dwellingTypeTesting,
type = "class")
# Display dwellingTypePrediction on the console
print(dwellingTypePrediction)
# Evaluate the model by forming a confusion matrix
dwellingTypeConfusionMatrix <- table(dwellingTypeTesting$DwellingType,
dwellingTypePrediction)
# Display the confusion matrix on the console
print(dwellingTypeConfusionMatrix)
# Calculate the model predictive accuracy and store it into a variable called
# predictiveAccuracy
dwellingTypeAccuracy <- sum(diag(dwellingTypeConfusionMatrix))/
nrow(dwellingTypeTesting)
# Display the predictive accuracy on the console
print(dwellingTypeAccuracy)
# Medha Singh
# Decision Tree
# This code demonstrates the summary of the IndonesianRiceFarms
# available in the form of a csv file. Performs data analysis for various
# features and displays DecisionTree visualizations.
# Installing rpart.plot packages
install.packages("rpart.plot")
# Loading tidyverse and rpart.plot packages
library(rpart.plot)
library(tidyverse)
# Setting current directory
setwd("C:/Users/ual-laptop/Documents/GitHub/DecisionTree")
# Loading data from csv into riceFarms
riceFarms <- read_csv(file = "IndonesianRiceFarms.csv",
col_types = "fnniinf",
col_names = TRUE
)
# Displaying structure of tibble
print(str(riceFarms))
# Displaying summary of tibble
print(summary(riceFarms))
# Generating 75% and 25% for Training and Testing
set.seed(370)
sampleSet <- sample(nrow(riceFarms),
round(nrow(riceFarms)*.75),
replace = FALSE)
riceFarmsTraining <- riceFarms[sampleSet,]
riceFarmsTesting <- riceFarms[-sampleSet,]
# Generating Decision Tree model for tibble
riceFarmDecisionTreeModel <- rpart(formula = FarmOwnership ~ .,
method = "class",
cp = 0.01,
data = riceFarmsTraining)
# Generating Rpart.plot
rpart.plot(riceFarmDecisionTreeModel)
# MEDHA SINGH
# Association Rule
# This code demonstrates the summary of InstacartTransactions.csv, display first
# 3 transactions, examine the frequency of an item, create the association rules
# model, and sort by lift parameter.
# Install tidyverse, arules packages -----------------------------
# install.packages("tidyverse")
# install.packages("arules")
# Load tidyverse, arules packages --------------------------------
library(tidyverse)
library(arules)
# Medha Singh
# K Means
# Description: Install and load required library. Summarise the data and
# generate clusters for the given data. Visualise and analyse the clusters.
# Optimize the value for k by evaluating the elbow method, average silhouette
# method, and gap statistic method. Visualise the final clusters. Lastly,
# Determine similarities and differences among the clusters using the remaining
# features in the dataset
# Install the tidyverse and factoextra packages
# install.packages("tidyverse")
# install.packages("factoextra")
# load the required library
library(tidyverse)
library(factoextra)
# Set the working directory to Lab11 folder
setwd("C:/Users/ual-laptop/Documents/GitHub/KMeans")
# Read CountryData.csv into a tibble called CountryData
CountryData <- read_csv(file = "CountryData.csv",
col_types = "cnnnnini",
col_names = TRUE)
# Display CountryData in the console
print(CountryData)
# Display the structure of CountryData in the console
print(str(CountryData))
# Display the summary of CountryData in the console
print(summary(CountryData))
# Convert the column containing the country name to the row title of the tibble
CountryData <- CountryData %>% column_to_rownames(var = "Country")
# Remove countries from the tibble with missing data in any feature
CountryData <- CountryData %>% drop_na()
# View the summary of the countries tibble again to ensure there are no NA
# values
print(summary(CountryData))
# Create a new tibble called countriesScaled containing only corruption index
# and the number of days it takes to open a business
countriesScaled <- CountryData %>% select(CorruptionIndex, DaysToOpenBusiness)
# scale them so they have equal impact on the clustering calculations
countriesScaled <- countriesScaled %>% scale()
# Set the random seed to 679
set.seed(679)
# Generate the k-means clusters in an object called countries4Clusters using 4
# clusters and a value of 25 for nstart
countries4Clusters <- kmeans(x = countriesScaled,
centers = 4,
nstart = 25)
# Display cluster sizes on the console
countries4Clusters$size
# Display cluster centers (z-scores) on the console
countries4Clusters$centers
# Visualize the clusters
fviz_cluster(object = countries4Clusters,
data = countriesScaled,
repel = FALSE)
# Visualize the clusters
fviz_cluster(object = countries3Clusters,
data = countriesScaled,
repel = FALSE)
# Medha Singh
# K Means
# Description: Install and load required library. Summarise the data and
# generate clusters for the given data. Visualise and analyse the clusters.
# Optimize the value for k by evaluating the elbow method, average silhouette
# method, and gap statistic method. Visualise the final clusters. Lastly,
# Determine similarities and differences among the clusters using the remaining
# features in the dataset
# Install the tidyverse and factoextra packages
# install.packages("tidyverse")
# install.packages("factoextra")
# load the required library
library(tidyverse)
library(factoextra)
# Set the working directory to Lab11 folder
setwd("C:/Users/ual-laptop/Documents/GitHub/KMeans")
# Read CountryData.csv into a tibble called CountryData
CountryData <- read_csv(file = "CountryData.csv",
col_types = "cnnnnini",
col_names = TRUE)
# Display CountryData in the console
print(CountryData)
# Display the structure of CountryData in the console
print(str(CountryData))
# Display the summary of CountryData in the console
print(summary(CountryData))
# Convert the column containing the country name to the row title of the tibble
CountryData <- CountryData %>% column_to_rownames(var = "Country")
# Remove countries from the tibble with missing data in any feature
CountryData <- CountryData %>% drop_na()
# View the summary of the countries tibble again to ensure there are no NA
# values
print(summary(CountryData))
# Create a new tibble called countriesScaled containing only corruption index
# and the number of days it takes to open a business
countriesScaled <- CountryData %>% select(CorruptionIndex, DaysToOpenBusiness)
# scale them so they have equal impact on the clustering calculations
countriesScaled <- countriesScaled %>% scale()
# Set the random seed to 679
set.seed(679)
# Generate the k-means clusters in an object called countries4Clusters using 4
# clusters and a value of 25 for nstart
countries4Clusters <- kmeans(x = countriesScaled,
centers = 4,
nstart = 25)
# Display cluster sizes on the console
countries4Clusters$size
# Display cluster centers (z-scores) on the console
countries4Clusters$centers
# Visualize the clusters
fviz_cluster(object = countries4Clusters,
data = countriesScaled,
repel = FALSE)
# Description: Install and load required library. Summarise the data and
# generate clusters for the given data. Visualise and analyse the clusters.
# Optimize the value for k by evaluating the elbow method, average silhouette
# method, and gap statistic method. Visualise the final clusters. Lastly,
# Determine similarities and differences among the clusters using the remaining
# features in the dataset
# Install the tidyverse and factoextra packages
# install.packages("tidyverse")
# install.packages("factoextra")
# load the required library
library(tidyverse)
library(factoextra)
# Set the working directory to Lab11 folder
setwd("C:/Users/ual-laptop/Documents/GitHub/KMeans")
# Read CountryData.csv into a tibble called CountryData
CountryData <- read_csv(file = "CountryData.csv",
col_types = "cnnnnini",
col_names = TRUE)
# Display CountryData in the console
print(CountryData)
# Display the structure of CountryData in the console
print(str(CountryData))
# Display the summary of CountryData in the console
print(summary(CountryData))
# Convert the column containing the country name to the row title of the tibble
CountryData <- CountryData %>% column_to_rownames(var = "Country")
# Remove countries from the tibble with missing data in any feature
CountryData <- CountryData %>% drop_na()
# View the summary of the countries tibble again to ensure there are no NA
# values
print(summary(CountryData))
# Create a new tibble called countriesScaled containing only corruption index
# and the number of days it takes to open a business
countriesScaled <- CountryData %>% select(CorruptionIndex, DaysToOpenBusiness)
# scale them so they have equal impact on the clustering calculations
countriesScaled <- countriesScaled %>% scale()
# Set the random seed to 679
set.seed(679)
# Generate the k-means clusters in an object called countries4Clusters using 4
# clusters and a value of 25 for nstart
countries4Clusters <- kmeans(x = countriesScaled,
centers = 4,
nstart = 25)
# Display cluster sizes on the console
countries4Clusters$size
# Display cluster centers (z-scores) on the console
countries4Clusters$centers
# Visualize the clusters
fviz_cluster(object = countries4Clusters,
data = countriesScaled,
repel = FALSE)
# Optimize the value for k
# elbow method
fviz_nbclust(x = countriesScaled,
FUNcluster = kmeans,
method = "wss")
# average silhouette method
fviz_nbclust(x = countriesScaled,
FUNcluster = kmeans,
method = "silhouette")
# gap statistic method
fviz_nbclust(x = countriesScaled,
FUNcluster = kmeans,
method = "gap_stat")
# Regenerate the cluster analysis using the optimal number of clusters
# Generate the k-means clusters in an object called countries3Clusters using 3
# clusters and a value of 25 for nstart
countries3Clusters <- kmeans(x = countriesScaled,
centers = 3,
nstart = 25)
# Display cluster sizes on the console
countries3Clusters$size
# Display cluster centers (z-scores) on the console
countries3Clusters$centers
# Visualize the clusters
fviz_cluster(object = countries3Clusters,
data = countriesScaled,
repel = FALSE)
# Determine similarities and differences among the clusters using the remaining
# features in the dataset (GiniCoefficient, GDPPerCapita, EduPercGovSpend,
# EduPercGDP, and CompulsoryEducationYears
clusterDiffSim <- CountryData %>%
mutate(cluster = countries3Clusters$cluster) %>%
select(cluster,
GiniCoefficient,
GDPPerCapita,
EduPercGovSpend,
EduPercGDP,
CompulsoryEducationYears) %>%
group_by(cluster) %>%
summarise_all("mean")
print(clusterDiffSim)
# Medha Singh
# Neural Network
# Description: Install and load the required library. Summarise the data
# and generate a neural network model and train to predict CharteredBoat.
# Plot the neural network. Generate probabilities for the testing set.
# Compute confusion
# matrix to measure the accuracy of the model
# Install the tidyverse and neuralnet packages
# install.packages("tidyverse")
# install.packages("neuralnet")
# load the required library
library(tidyverse)
library(neuralnet)
# Set the working directory to Lab11 folder
setwd("C:/Users/ual-laptop/Documents/GitHub/NeuralNetwork")
# Read FishingCharter.csv into a tibble called fishingCharter
fishingCharter <- read_csv(file = "FishingCharter.csv",
col_types = "lnn",
col_names = TRUE)
# Display fishingCharter in the console
print(fishingCharter)
# Display the structure of fishingCharterin the console
print(str(fishingCharter))
# Display the summary of fishingCharter in the console
print(summary(fishingCharter))
# Scale the AnnualIncome and CatchRate variables
fishingCharter <- fishingCharter %>%
mutate(AnnualIncomeScaled = (AnnualIncome - min(AnnualIncome))/
(max(AnnualIncome) - min(AnnualIncome)))
fishingCharter <- fishingCharter %>%
mutate(CatchRateScaled = (CatchRate - min(CatchRate))/
(max(CatchRate) - min(CatchRate)))
# Randomly split the dataset into fishingCharterTraining (75% of records)
# and fishingCharterTesting (25% of records) using 591 as the random seed
set.seed(591)
sampleSet <- sample(nrow(fishingCharter),
round(nrow(fishingCharter)*0.75),
replace = FALSE)
fishingCharterTraining <- fishingCharter[sampleSet,]
fishingCharterTesting <- fishingCharter[-sampleSet,]
# Generate the neural network model to predict CharteredBoat (dependent
# variable) using AnnualIncomeScaled and CatchRateScaled (independent
# variables). Use 3 hidden layers. Use "logistic" as the smoothing method
# and set linear. Output to FALSE.
fishingCharterNeuralNet <- neuralnet(
formula = CharteredBoat ~ AnnualIncomeScaled + CatchRateScaled,
data = fishingCharterTraining,
hidden = 3,
act.fct = "logistic",
linear.output = FALSE
)
# Display the neural network numeric results
print(fishingCharterNeuralNet$result.matrix)
# Visualize the neural network
plot(fishingCharterNeuralNet)
# Use fishingCharterNeuralNet to generate probabilities on the
# fishingCharterTesting data set and store this in
fishingCharterProbability
# Visualize the neural network
plot(fishingCharterNeuralNet)
# Use fishingCharterNeuralNet to generate probabilities on the
# fishingCharterTesting data set and store this in
fishingCharterProbability
fishingCharterProbability <- compute(fishingCharterNeuralNet,
fishingCharterTesting)
# Display the probabilities from the testing dataset on the console
print(fishingCharterProbability$net.result)
# Convert probability predictions into 0/1 predictions and store this
# into fishingCharterPrediction
fishingCharterPrediction <-
ifelse(fishingCharterProbability$net.result > 0.5, 1, 0)
# Display the 0/1 predictions on the console
print(fishingCharterPrediction)
# Evaluate the model by forming a confusion matrix
fishingCharterCinfusionMatrix <-
table(fishingCharterTesting$CharteredBoat,
fishingCharterPrediction)
# Display the confusion matrix on the console
print(fishingCharterCinfusionMatrix)
# Calculate the model predictive accuracy
predictiveAccuracy <- sum(diag(fishingCharterCinfusionMatrix)) /
nrow(fishingCharterTesting)
# Display the predictive accuracy on the console
print(predictiveAccuracy)
